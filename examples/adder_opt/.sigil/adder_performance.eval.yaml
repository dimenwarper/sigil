version: '0.1'
name: adder_performance
description: Measure correctness and latency of add_all
inputs:
  generator: null
metrics:
- id: correctness
  kind: checker
  command: python3 benchmark.py correctness
  parse: exit_code==0
- id: latency_ms
  kind: numeric
  command: python3 benchmark.py latency
  parse: "latency_ms=([0-9.]+)"
aggregate:
  objective: min(latency_ms) subject_to correctness==true
  tie_breakers:
  - mean(latency_ms)
accept:
  rule: correctness==true
budgets:
  candidate_timeout_s: 60
  total_wall_clock_h: 1
replay:
  seed: 7
